\section{Model}
\label{sec:Framework}
To formalize how an agent allocates effort between tasks, consider an agent with a fixed cognitive bandwidth $C\in\mathbb{R}_{++}$ \citep{simon1955behavioral,kahneman1973attention,norman1975data,navon1979economy,sims2003implications}. This agent allocates scarce cognitive resources between two competing tasks (Task 1 and Task 2) analogous to a dual-task paradigm studied extensively in psychology \citep{pashler1994dual} and economics \citep{holmstrom1991multitask,buser2012multitasking}. In Task 1, the agent may complete the task themselves with effort $a_1\in\mathbb{R}_+$, or deploy up to $n\in\mathbb{N}$ AI systems to complete Task 1 on their behalf, but must allocate resources $\mathbf{e}=[e_1,...,e_n]\in\mathbb{R}^n_{+}$ to write and refine prompts. The effort $\mathbf{e}$ directed towards enhancing the $n$ AI systems on Task 1 generates a product which the unassisted agent could've completed with $\alpha=f(\mathbf{e})$ units of effort s.t. $\nabla f(\mathbf{e})=[\frac{\partial f(\mathbf{e})}{\partial e_1},...,\frac{\partial f(\mathbf{e})}{\partial e_n}]^t\in\mathbb{R}^n_{++}$. At the same time, the agent must complete Task 2 on their own with the effort $a_2\in\mathbb{R}_{+}$. In both tasks, the agent derives utility from their performance. Task 1 \& 2 performances are denoted as $p_1=a_1+\alpha$ \& $p_2=a_2$, respectively, according to the utility function $U(p_1,p_2)$ \citep{drichoutis2020economic} which is assumed to be twice differentiable, strictly increasing, and quasi-concave. It follows that the decision problem faced by the agent is given by expression~\ref{eq:problem}:

\begin{equation}\label{eq:problem}
\begin{aligned}
\max_{a_1,\mathbf{e},a_2}\quad
& U\bigl(a_1 + f(\mathbf{e}),\, a_2\bigr)\\[6pt]
\text{s.t.}\quad
& a_1 + \sum_{i=1}^{n} e_i + a_2 \;=\; C,\\
& a_1,\,e_i,\,a_2 \;\ge\; 0,\quad i = 1,\dots,n.
\end{aligned}
\end{equation}

For the model illustration, all `prices' of effort are normalized to 1 in the `budget' constraint $a_1 + \sum_{i=1}^{n} e_i + a_2 = C$ as all forms of cognitive effort are ultimately measured in uniform ``units" drawn from the same finite resource pool (e.g. attention, working memory, mental bandwidth, etc.). This is supported by foundational work in cognitive psychology and economics on attention and working memory \citep{kahneman1973attention, baddeley1992working}, data-limited and resource-limited processes \citep{norman1975data}, and the concept of cognitive bandwidth as an economic resource \citep{navon1979economy, caplin2016measuring, loewenstein2023economics}. Fundamentally, the effort exerted to write and refine AI prompts is equivalent to the effort exerted to perform the task directly because both draw from the same limited cognitive reserves \citep{zamfirescu2023johnny}. In other words, whether an agent exerts effort crafting a prompt to elicit a desired response from an AI or is manually performing the task unassisted, both utilizing activities draw from the same constrained mental resources $C$, similar to those outlined in the limited resource model \citep{norman1975data} or in multi-task settings \citep{borghini2012assessment}. This approach mirrors standard microeconomic theory, where diverse resources are often aggregated into a single budget constraint and assigned a unitary ``price" for analytical tractability, simplifying the analysis without loss of generality \citep{becker1965theory,varian1992microeconomic}. By expressing all activities in terms of a common pool resource, the model focuses on the essential trade-offs between competing uses of effort and attention, rather than on arbitrary scaling factors that do not affect the underlying optimization problem \citep{RePEc:oxp:obooks:9780195102680}. Therefore, the cognitive effort expended to improve an AI system is directly comparable to the effort required to complete the task independently, as they are essentially interchangeable claims on the same constrained cognitive resource.

This analysis examines when the choice variables are strictly positive for which the decision problem~\ref{eq:problem} has an \textit{interior solution}. Notably, when focusing on an interior solution, standard first-order optimality conditions often suffice to characterize how the agent distributes resources across tasks. Intuitively, as AI systems become increasingly advanced, corresponding to a larger $\nabla f(\mathbf{e})$, people will integrate more AI enhancements into their workflows. This increase in productivity corresponds to a greater incentive for having AI systems assist with all of the work on people's behalf. However, a drawback of reallocating cognitive resources from completing the task unassisted to improving AI systems is the reduced oversight of the latent process determining the observed performance. As people spend a greater amount of time improving the AI system to completing a task on their behalf, rather than improving their own performance on the task, there will be reduced human oversight to ensure the validity of the final product. In essence, an interior solution in the case of the model's framework guarantees that the agent leveraging AI (i.e., $\mathbf{e}^*>0$) has \textit{some} control over the process in Task 1 with their input $a_1^*>0$, while also attending to tasks which cannot be replaced with AI with their input $a^*_2>0$.

Further strengthening this concept, the interior solution of the decision problem~\ref{eq:problem} can be solved using the Lagrangian multiplier method. Proposition~\ref{pro:foc} illustrates the first-order condition of the agent's decision problem~\ref{eq:problem}, providing insight into how people optimize the use of AI while ensuring that its work is accurate.

\begin{proposition}
\label{pro:foc}
If the allocation $(a^*_1,\mathbf{e}^*,a^*_2)$ is a unique interior stationary point to the decision problem~\ref{eq:problem}, then the marginal product of enhancing $n$ AI systems equals the marginal product of completing Task 1 unassisted, i.e.
$$\nabla f(\mathbf{e}^*)=\begin{bmatrix}
1\\
...\\
1
\end{bmatrix}$$
\end{proposition}
\begin{proof}
Provided in Appendix~\ref{app:foc}.
\end{proof}

Due to chain rule, the optimal amount of effort to allocate towards AI is independent of the agent's preferences. Intuitively, since the only payoff-relevant judgment is on the performances $p_i$, the agent is indifferent to the origins of their judged performance beyond how much effort they must allocate to achieve it. For example, if a contracted developer is paid for the quality of a website, their payment doesn't depend on whether the source code was written by themselves, a sub-contractor, or generated by AI. Rather, the developer is judged based on the functionality and aesthetics of the website. Indeed, the decision problem~\ref{eq:problem} is an optimization of the \textit{process}, not the \textit{outcome}. In other words, the AI production function $f$, not $U$, determines the agent's allocation policy for prompting AI systems and completing Task 1 unassisted. The production function $f$ depends on both the AI system and the prompting ability of the agent. An increase in $\nabla f$ could correspond to the agent becoming better at prompting the AI systems to contribute more to their observed performance in Task 1, but also the AI systems becoming better at contributing the agent's Task 1 performance for the same prompt.

Like the first-order condition of an interior solution outlined in Proposition~\ref{pro:foc} determined by the AI production function $f$, the second-order condition is determined by the curvature of $f$. 

\begin{proposition}
\label{pro:soc}
The first-order condition outlined in Proposition~\ref{pro:foc} is a maximum if the agent faces diminishing returns with the $n$ AI systems:
$$D^2f(\mathbf{e}^*)<0\quad$$
\end{proposition}
\begin{proof}
Provided in Appendix~\ref{app:soc}.
\end{proof}

In particular, Proposition~\ref{pro:soc} shows that the AI production function \textit{must} exhibit diminishing returns for $(a^*_1,\mathbf{e}^*,a^*_2)$ to be unique. Intuitively, if strict concavity of $f$ doesn't hold at the stationary point $(a^*_1,\mathbf{e}^*,a^*_2)$, one of two cases must be true: the agent exhibits (1)  increasing returns to scale or (2) constant returns to scale of enhancing the $n$ AI systems. If the marginal product of AI enhancement was increasing, the agent would always have the incentive to reallocate direct effort on Task 1 ($a^*_1$) towards AI enhancement $\mathbf{e}^*$ until $a^*_1=0$.\footnote{Since $U$ is strictly increasing in Task 1 and Task 2 performance, $\nabla f(\mathbf{e}^*)=1$ and $D^2 f(\mathbf{e}^*)>0$ implies that $\tilde{a}_1+f(\tilde{\mathbf{e}})>a_1^*+f({\mathbf{e}^*})\iff U(\tilde{a}_1+f(\tilde{\mathbf{e}}),a_2^*)>U({a}_1^*+f({\mathbf{e}}^*),a_2^*)$ for each $\tilde{\mathbf{e}}>\mathbf{e}$ such that $\tilde{\mathbf{e}}\in\mathbb{R}^n_+$ is a feasible allocation (i.e. $C=\sum_{i=1}^n{e}^*_i+{a}^*_1=\sum_{i=1}^n\tilde{e}_i+\tilde{a}_1$) and for a \textit{fixed} Task 2 effort level $a^*_2$ across the two feasible allocations. Since $(a^*_1,\mathbf{e}^*,a^*_2)$ is a unique maximum in the feasible set and $U(\tilde{a}_1+f(\tilde{\mathbf{e}}),a_2^*)>U({a}_1^*+f({\mathbf{e}}^*),a_2^*)$ for some feasible allocation $(\tilde{a}_1,\tilde{\mathbf{e}},{a}_2^*)$, a contradiction arises. Therefore, the AI production function $f$ cannot (1) exhibit increasing returns to scale and (2) have the interior point satisfying Proposition~\ref{pro:foc} be utility maximizing simultaneously.} Conversely, if the marginal product of AI enhancement was constant, the agent would either completely ignore the AI systems and complete the task on their own [i.e., $\frac{\partial f(\mathbf{e})}{\partial e_i}=c<1$ $\forall e_i\in (0,C)$ and each AI system $i\in\{1,...,n\}$], be indifferent between leveraging AI and completing the task themselves since there would exist at least one AI system $j\in\{1,...,n\}$ such that $\frac{\partial f(\mathbf{e})}{\partial e_j}=1$ $\forall e_j\in(0,C)$ and $\frac{\partial f(\mathbf{e})}{\partial e_j}\geq\frac{\partial f(\mathbf{e})}{\partial e_i}$ $\forall j\neq i$, or utilize the AI to complete all of Task 1 on their behalf since there would be at least one AI system $ k\in\{1,...,n\}\text{ s.t. }\frac{\partial f(\mathbf{e})}{\partial e_k}=c>1$ $\forall e_k\in (0,C)$. All of these cases lead to non-interior solutions, meaning that $f$ cannot be linear or strictly convex so thus must be strictly concave.

Proposition~\ref{pro:soc}, notably, doesn't depend on the substituability or complementariness of the $n$ AI systems being deployed to complete Task 1 on the agent's behalf. To demonstrate this point, consider the second-order conditions when the agent deploys $n=2$ AI systems: (1) $\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1^2}<0$ and (2) $\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1^2}*\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_2^2}-\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1\partial e_2}*\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_2\partial e_1}>0$. Here, the substituability of the two AI systems depends on the cross-partial derivatives of $f$ \citep{RePEc:oxp:obooks:9780195102680}, so the examination may ignore condition (1). For condition (2), Young's theorem stipulates the cross partials of $e_1$ and $e_2$ must equal, meaning the condition can be rewritten as $\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1^2}*\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_2^2}-\bigg(\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1\partial e_2}\bigg)^2>0$. Because $\bigg(\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1\partial e_2}\bigg)^2=\bigg(-\frac{\partial^2 f(\mathbf{e}^*)}{\partial e_1\partial e_2}\bigg)^2$, the \textit{sign} of the cross-partial derivative does not affect the negative definiteness of the Hessian matrix. In essence, the second-order conditions outlined in Proposition~\ref{pro:soc} are independent of whether the AI systems deployed are complements or substitutes.